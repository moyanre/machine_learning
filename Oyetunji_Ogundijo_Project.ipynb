{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing Relevant Libraries\n",
    "\n",
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# Data processing\n",
    "import pandas as pd \n",
    "\n",
    "# Algorithms and Metric\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Data Visualization \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing the Data \n",
    "all_data = pd.read_table(\"DScasestudy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Exploration of the Data:\n",
    "\n",
    "After importing the data, I looked at the necessary information about the data. The data comprised of 530 rows\n",
    "of observations, 1 column of response variable and 16562 columns of features. With the very large number of features\n",
    "compared to the number of observation, it is imperative that I look deeper into the features and see if there are \n",
    "ways to reduce the number of features so that the model does not suffer from overfitting.\n",
    " \n",
    "Next, I looked at the different statistics for each feature and the response to have a deeper and clearer \n",
    "understanding of the data. I immediately noticed that some of the features in the data have the same value for all \n",
    "the observations. This implies that with proper feature selection methods, the features can be reduced.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 530 entries, 0 to 529\n",
      "Columns: 16563 entries, response to V16562\n",
      "dtypes: int64(16563)\n",
      "memory usage: 67.0 MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info() # data type, memory information and index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V16553</th>\n",
       "      <th>V16554</th>\n",
       "      <th>V16555</th>\n",
       "      <th>V16556</th>\n",
       "      <th>V16557</th>\n",
       "      <th>V16558</th>\n",
       "      <th>V16559</th>\n",
       "      <th>V16560</th>\n",
       "      <th>V16561</th>\n",
       "      <th>V16562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   response  V1  V2  V3  V4  V5  V6  V7  V8  V9   ...    V16553  V16554  \\\n",
       "0         1   0   0   0   0   0   1   0   0   0   ...         1       0   \n",
       "1         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "2         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "3         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "4         1   0   0   0   0   0   1   0   0   0   ...         0       0   \n",
       "\n",
       "   V16555  V16556  V16557  V16558  V16559  V16560  V16561  V16562  \n",
       "0       0       0       0       0       0       0       0       0  \n",
       "1       0       0       0       0       0       0       0       1  \n",
       "2       0       0       0       0       0       0       0       1  \n",
       "3       0       0       0       0       0       0       0       0  \n",
       "4       0       0       0       0       0       0       0       0  \n",
       "\n",
       "[5 rows x 16563 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V16553</th>\n",
       "      <th>V16554</th>\n",
       "      <th>V16555</th>\n",
       "      <th>V16556</th>\n",
       "      <th>V16557</th>\n",
       "      <th>V16558</th>\n",
       "      <th>V16559</th>\n",
       "      <th>V16560</th>\n",
       "      <th>V16561</th>\n",
       "      <th>V16562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>...</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>530.000000</td>\n",
       "      <td>530.0</td>\n",
       "      <td>530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.232075</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.030189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>0.013208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.422556</td>\n",
       "      <td>0.096761</td>\n",
       "      <td>0.096761</td>\n",
       "      <td>0.043437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043437</td>\n",
       "      <td>0.171268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043437</td>\n",
       "      <td>0.142697</td>\n",
       "      <td>0.114270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.497671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 16563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         response          V1          V2          V3     V4          V5  \\\n",
       "count  530.000000  530.000000  530.000000  530.000000  530.0  530.000000   \n",
       "mean     0.232075    0.009434    0.009434    0.001887    0.0    0.020755   \n",
       "std      0.422556    0.096761    0.096761    0.043437    0.0    0.142697   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.0    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.0    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.0    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.0    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    0.0    1.000000   \n",
       "\n",
       "          V6          V7          V8     V9     ...          V16553  V16554  \\\n",
       "count  530.0  530.000000  530.000000  530.0     ...      530.000000   530.0   \n",
       "mean     1.0    0.001887    0.030189    0.0     ...        0.158491     0.0   \n",
       "std      0.0    0.043437    0.171268    0.0     ...        0.365545     0.0   \n",
       "min      1.0    0.000000    0.000000    0.0     ...        0.000000     0.0   \n",
       "25%      1.0    0.000000    0.000000    0.0     ...        0.000000     0.0   \n",
       "50%      1.0    0.000000    0.000000    0.0     ...        0.000000     0.0   \n",
       "75%      1.0    0.000000    0.000000    0.0     ...        0.000000     0.0   \n",
       "max      1.0    1.000000    1.000000    0.0     ...        1.000000     0.0   \n",
       "\n",
       "       V16555      V16556      V16557      V16558  V16559      V16560  V16561  \\\n",
       "count   530.0  530.000000  530.000000  530.000000   530.0  530.000000   530.0   \n",
       "mean      0.0    0.001887    0.020755    0.013208     0.0    0.003774     0.0   \n",
       "std       0.0    0.043437    0.142697    0.114270     0.0    0.061371     0.0   \n",
       "min       0.0    0.000000    0.000000    0.000000     0.0    0.000000     0.0   \n",
       "25%       0.0    0.000000    0.000000    0.000000     0.0    0.000000     0.0   \n",
       "50%       0.0    0.000000    0.000000    0.000000     0.0    0.000000     0.0   \n",
       "75%       0.0    0.000000    0.000000    0.000000     0.0    0.000000     0.0   \n",
       "max       0.0    1.000000    1.000000    1.000000     0.0    1.000000     0.0   \n",
       "\n",
       "           V16562  \n",
       "count  530.000000  \n",
       "mean     0.447170  \n",
       "std      0.497671  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "\n",
       "[8 rows x 16563 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.describe()  # data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the original data, there are 530 rows and 16563 columns.\n",
      "Alternatively, there are 530 observations and 16562 features.\n"
     ]
    }
   ],
   "source": [
    "dimm = all_data.shape\n",
    "ro = 'In the original data, there are %d rows and %d columns.' %(dimm[0], dimm[1])\n",
    "ro1 = 'Alternatively, there are %d observations and %d features.' %(dimm[0], dimm[1]-1)\n",
    "print(ro)\n",
    "print(ro1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Cleaning:\n",
    "\n",
    "Although uncommon, I started the data cleaning process by first checking, if available, any observation with a null\n",
    "value as the response and I found none.\n",
    "Next, I checked if there were null value(s) for any feature in any of the observations. Surprisingly, none was found.\n",
    "This means that the data has no null value at all.\n",
    "\n",
    "Next, I looked at the 'prior distribution' of classes. I found that there is class imbalance. specifically, just \n",
    "23 percent of the observations belong to the class with label '1'. With this discovery, it will be inappropriate to\n",
    "employ 'accuracy' as the metric to measure the performance of the machine learning models. Reason being that if a \n",
    "particular classifier predicts only class '0', its accuracy will be 78 percent (null accuracy). Hence, I resolved \n",
    "to employ a better metric that will account for the inherent class imbalance in the data. Specifically, I made use \n",
    "of the Area Under Curve (AUC) of Receiver Operating Characteristic (ROC) to adjudge the performance of the models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 530 observations and 16562 features after checking for the presence of Null response.\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data[np.isfinite(all_data['response'])] # drop all rows where the response is null (if applicable)\n",
    "dimm1 = all_data.shape\n",
    "ro2 = 'There are %d observations and %d features after checking for the presence of Null response.' %(dimm1[0], dimm1[1]-1)\n",
    "print(ro2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Null value\n"
     ]
    }
   ],
   "source": [
    "# Null check\n",
    "def null_check(data):\n",
    "    ind = np.sum(data.isnull().sum())\n",
    "    if ind == 0:\n",
    "        print('No Null value')\n",
    "    else:\n",
    "        print('There is/are Null value(s)')\n",
    "\n",
    "null_check(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of observations in class 1 is 23%.\n"
     ]
    }
   ],
   "source": [
    "## Class Imbalance\n",
    "per_value = all_data['response'].sum()/all_data['response'].count() * 100\n",
    "to1 = \"Percentage of observations in class 1 is %d\" %per_value \n",
    "print(to1,'%.', sep = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.bar(['0','1'], [all_data['response'].count() - all_data['response'].sum(), all_data['response'].sum()])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count of Class Observations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Because the percentage of class label 1 is 23%, the class is imbalance. I considered AUC-ROC curve as the metric\n",
    "of interest.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature Selection:\n",
    "\n",
    "Selection of relevant features is inevitable in this kind of problem owing to the large number of features. However,\n",
    "before such an operation, I split the data into training and test sets. Feature selection will be done on the training\n",
    "set and the selected features will be used to transform/filter the two sets as. By doing the spliting before \n",
    "selection, overfitting will be avoided. As such, the data was split in the ratio 4:1 for the training and test sets,\n",
    "respectively.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y =train_test_split(all_data.drop(labels=['response'], axis=1),\n",
    "                                                   all_data['response'],test_size=0.2,random_state = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V16553</th>\n",
       "      <th>V16554</th>\n",
       "      <th>V16555</th>\n",
       "      <th>V16556</th>\n",
       "      <th>V16557</th>\n",
       "      <th>V16558</th>\n",
       "      <th>V16559</th>\n",
       "      <th>V16560</th>\n",
       "      <th>V16561</th>\n",
       "      <th>V16562</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 16562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     V1  V2  V3  V4  V5  V6  V7  V8  V9  V10   ...    V16553  V16554  V16555  \\\n",
       "356   0   0   0   0   0   1   0   0   0    0   ...         0       0       0   \n",
       "460   0   0   0   0   0   1   0   0   0    0   ...         0       0       0   \n",
       "106   0   0   0   0   0   1   0   0   0    0   ...         0       0       0   \n",
       "479   0   0   0   0   0   1   0   0   0    0   ...         0       0       0   \n",
       "264   0   0   0   0   0   1   0   0   0    0   ...         0       0       0   \n",
       "\n",
       "     V16556  V16557  V16558  V16559  V16560  V16561  V16562  \n",
       "356       0       0       0       0       0       0       1  \n",
       "460       0       0       0       0       0       0       1  \n",
       "106       0       0       0       0       0       0       0  \n",
       "479       0       0       0       0       0       0       1  \n",
       "264       0       0       0       0       0       0       1  \n",
       "\n",
       "[5 rows x 16562 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of observations in class 1 in the training set is 22%.\n"
     ]
    }
   ],
   "source": [
    "per_train = train_y.sum()/train_y.count() * 100\n",
    "to2 = \"Percentage of observations in class 1 in the training set is %d\" %per_train\n",
    "print(to2,'%.', sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of observations in class 1 in the test set is 27%.\n"
     ]
    }
   ],
   "source": [
    "per_test = test_y.sum()/test_y.count() * 100\n",
    "to3 = \"Percentage of observations in class 1 in the test set is %d\" %per_test\n",
    "print(to3,'%.', sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Steps of Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removing constant features: \n",
    "Constant features are those features that have the same value in every observation in the training set. \n",
    "They do not vary among observations (i.e., their variance is 0).\n",
    "They are removed as they do not contribute information to predicting the response variable. \n",
    "Here, all features with constant value 0 or 1 in all the observations were dropped.\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6129 features have constant values and will subsequently be removed from the training and the test sets.\n",
      "Now, the dimension of the training set is (424, 10433)\n",
      "and the dimension of the test set is (106, 10433)\n"
     ]
    }
   ],
   "source": [
    "constant_remover = VarianceThreshold(threshold=0) \n",
    "constant_remover.fit(train_X)\n",
    "picked_columns = train_X.columns[constant_remover.get_support()]\n",
    "print('%d features have constant values and will subsequently be removed from the training and the test sets.'\n",
    "      %(len(train_X.columns) - len(picked_columns)))\n",
    "train_X = constant_remover.transform(train_X)\n",
    "test_X = constant_remover.transform(test_X)\n",
    "train_X = pd.DataFrame(train_X, columns = picked_columns)\n",
    "test_X = pd.DataFrame(test_X, columns = picked_columns)\n",
    "print('Now, the dimension of the training set is', train_X.shape)\n",
    "print('and the dimension of the test set is', test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removing quasi-constant features\n",
    "Next, quasi-constant features will be removed. i.e., those below certain threshold of variance. These features do \n",
    "not vary much among the observations in the training set.\n",
    "Feature that have variance below 0.01 were removed.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3919 features have variance below 0.01 and will subsequently be removed from the training and the test sets.\n",
      "Now, the dimension of training set is (424, 6514)\n",
      "and the dimension of test set is (106, 6514)\n"
     ]
    }
   ],
   "source": [
    "qconstant_removal = VarianceThreshold(threshold=0.01) \n",
    "qconstant_removal.fit(train_X) \n",
    "picked_columns_quasi = train_X.columns[qconstant_removal.get_support()]\n",
    "print('%d features have variance below 0.01 and will subsequently be removed from the training and the test sets.'\n",
    "      %(len(train_X.columns) - len(picked_columns_quasi)))\n",
    "train_X = qconstant_removal.transform(train_X)\n",
    "test_X = qconstant_removal.transform(test_X)\n",
    "train_X = pd.DataFrame(train_X, columns = picked_columns_quasi)\n",
    "test_X = pd.DataFrame(test_X, columns = picked_columns_quasi)\n",
    "print('Now, the dimension of training set is', train_X.shape)\n",
    "print('and the dimension of test set is', test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removing duplicate features and leaving one and only one 'copy'\n",
    "Duplicate features are removed because having so many duplicates of a particular feature does not add any value \n",
    "to algorithm training. Rather, they add overhead and unnecessary delay to the training time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424 entries, 0 to 423\n",
      "Columns: 6514 entries, V5 to V16562\n",
      "dtypes: int64(6514)\n",
      "memory usage: 21.1 MB\n"
     ]
    }
   ],
   "source": [
    "train_X = train_X.T.drop_duplicates(keep = 'first').T\n",
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No duplicate features found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Looking at features that are highly correlated.\n",
    "Next, looking at the remaining 6514 features to see if two or more than two of these features are highly \n",
    "correlated, i.e., if they are close together in the linear space. If such correlation occurs, it means they carry \n",
    "redundant information to the model to be trained. Only one of such correlated features would be retained.\n",
    "Specifically, correlation between two features is computed. If the absolute value is greater than 0.8 (a relatively\n",
    "high value considering that the maximum possible correlation value is 1), one of the features is dropped.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 features will be removed as a result of being correlated with other features in the training set.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424 entries, 0 to 423\n",
      "Columns: 6449 entries, V5 to V16562\n",
      "dtypes: int64(6449)\n",
      "memory usage: 20.9 MB\n"
     ]
    }
   ],
   "source": [
    "corr_features_set = set()  \n",
    "corr_matrix = train_X.corr()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            col_name = corr_matrix.columns[i]\n",
    "            corr_features_set.add(col_name)\n",
    "            \n",
    "print('%d features will be removed as a result of being correlated with other features in the training set.' \n",
    "      %len(corr_features_set))\n",
    "train_X.drop(labels=corr_features_set, axis=1, inplace=True)  \n",
    "test_X.drop(labels=corr_features_set, axis=1, inplace=True)\n",
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From this point, two additional data preprocessing schemes were considered:\n",
    "1. standardization, such that the mean of each feature will be centered at 0 and the variance will be unity.\n",
    "  NB. Also, this will only be performed on the training set, but will be used to tranform both the training and the\n",
    "      test sets. \n",
    "2. Feature extraction using the principal component analysis (PCA). This is done to further reduce the number of \n",
    "   features. Summarily, the new (reduced) features that were extracted from the original features are basically\n",
    "  orthogonal or uncorrelated and they are linear combinations of the original features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Centering and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seunfunmi/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/seunfunmi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/seunfunmi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X_scaled = scaler.transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Extraction with Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Scaled Data + PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_X)\n",
    "train_X_pca = pca.transform(train_X)\n",
    "test_X_pca = pca.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaled Data + PCA\n",
    "pca_1 = PCA()\n",
    "pca_1.fit(train_X_scaled)\n",
    "train_X_scaled_pca = pca_1.transform(train_X_scaled)\n",
    "test_X_scaled_pca = pca_1.transform(test_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########   Building Machine Learning Models   ##############\n",
    "\n",
    "'''\n",
    "In building the Models, I made use of 4 different sets of data:\n",
    "1. train_X and test_X: The data obtained after feature selection.\n",
    "2. train_X_scaled and test_X_scaled: The data obtained from feature selection, centering and scaling.\n",
    "3. train_X_pca and test_X_pca: The data obtained from feature selection and PCA.\n",
    "4. train_X_scaled_pca and test_X_scaled_pca: The data obtained after feature selection, centering and scaling and PCA.\n",
    "\n",
    "Different classification models were trained on these sets:\n",
    "1. Logistic regression\n",
    "2. Random forest\n",
    "3. Support vector machine\n",
    "4. Naive Bayes\n",
    "5. K-Nearest neighbor\n",
    "\n",
    "\n",
    "The best model and data processing combination is \n",
    "selected, i.e., Feature Selection + Logistic Regression\n",
    "\n",
    "Logistic regression:\n",
    "A discriminative classifier that learns, from the training data, the weights that are associated with every feature.\n",
    "These weights specify (describe) the importance of each feature in predicting the response. Because the \n",
    "predicted output is the logistic function value of the dot product of all the features and the associated weights\n",
    "for a particular observation, the original result is often between 0 and 1. Values between 0 and 0.5 belong to one \n",
    "class and values greater than 0.5 belong to the other class.\n",
    "The regularized logistic regression considered often takes care of overfitting. \n",
    "\n",
    "\n",
    "Performance measurement: \n",
    "Area Under the Curve(AUC) of Receiver Operating Characteristics (ROC) curve.\n",
    "The AUC-ROC curve is constructed from the True Positive Rate (TPR) and False Positive Rate (FPR) at different\n",
    "thresholds. The maximum value is 1. The higher the value of the area under this curve, the better the predictive \n",
    "capability of the model. One important feature of this metric is its insensitivity to class imbalance, which is \n",
    "inherent in this data.\n",
    "\n",
    "The approximately 0.8 value of AUC-ROC from Feature Selection + logistic regression model indicates \n",
    "that there is 80% chance that the model will be able to distinguish between the two classes (class 0 and class 1).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rf = [train_X, train_X_scaled, train_X_pca, train_X_scaled_pca]\n",
    "test_rf = [test_X, test_X_scaled, test_X_pca, test_X_scaled_pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "logreg = LogisticRegression(solver = 'liblinear', class_weight = 'balanced')\n",
    "A_log = []\n",
    "for j in range(4):\n",
    "    trainn_X, testt_X = train_rf[j], test_rf[j]\n",
    "    logreg.fit(trainn_X, train_y)\n",
    "    pred_y_logi = logreg.predict(testt_X)\n",
    "    AUROC_score = roc_auc_score(test_y, pred_y_logi)\n",
    "    A_log.append(AUROC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=200, random_state = 55)\n",
    "A_rf = []\n",
    "for j in range(4):\n",
    "    trainn_X, testt_X = train_rf[j], test_rf[j]\n",
    "    random_forest.fit(trainn_X, train_y)\n",
    "    pred_y_rf = random_forest.predict(testt_X)\n",
    "    AUROC_score = roc_auc_score(test_y, pred_y_rf)\n",
    "    A_rf.append(AUROC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear SVC\n",
    "linear_svc = LinearSVC()\n",
    "A_svm = []\n",
    "for j in range(4):\n",
    "    trainn_X, testt_X = train_rf[j], test_rf[j]\n",
    "    linear_svc.fit(trainn_X, train_y)\n",
    "    pred_y_svm = linear_svc.predict(testt_X)\n",
    "    AUROC_score = roc_auc_score(test_y, pred_y_svm)\n",
    "    A_svm.append(AUROC_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gaussian = GaussianNB()\n",
    "A_gauss = []\n",
    "for j in range(4):\n",
    "    trainn_X, testt_X = train_rf[j], test_rf[j]\n",
    "    gaussian.fit(trainn_X, train_y)\n",
    "    pred_y_gauss = gaussian.predict(testt_X)\n",
    "    AUROC_score = roc_auc_score(test_y, pred_y_gauss)\n",
    "    A_gauss.append(AUROC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 3) \n",
    "A_knn = []\n",
    "for j in range(4):\n",
    "    trainn_X, testt_X = train_rf[j], test_rf[j]\n",
    "    knn.fit(trainn_X, train_y)\n",
    "    pred_y_knn = knn.predict(testt_X)\n",
    "    AUROC_score = roc_auc_score(test_y, pred_y_knn)\n",
    "    A_knn.append(AUROC_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Not Scaled    Scaled  Not Scaled + PCA  Scaled + PCA\n",
      "Logistic Regression        0.790864  0.755934          0.771384      0.755934\n",
      "Random Forest              0.672414  0.672414          0.500000      0.500000\n",
      "Support Vector Machines    0.756382  0.762427          0.784371      0.762427\n",
      "Gaussian Naive Bayes       0.525750  0.525750          0.581729      0.570981\n",
      "KNN                        0.555979  0.500000          0.549485      0.500000\n"
     ]
    }
   ],
   "source": [
    "### Results of AUC-ROC for all the models considered is shown here. Feature Selection + Logistic \n",
    "## Regression showed the best performance with AUC of 0.8.\n",
    "\n",
    "Models = ['Logistic Regression','Random Forest','Support Vector Machines','Gaussian Naive Bayes', 'KNN']\n",
    "cols = ['Not Scaled', 'Scaled', 'Not Scaled + PCA', 'Scaled + PCA']\n",
    "res_arr = [A_log, A_rf, A_svm, A_gauss, A_knn]\n",
    "\n",
    "results = pd.DataFrame(res_arr, index = Models, columns = cols)\n",
    "print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
